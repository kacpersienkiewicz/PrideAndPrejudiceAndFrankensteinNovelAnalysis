{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Text of *Frankenstein* and *Pride and Prejudice*\n",
    "\n",
    "This project will use the  **Natural Language Toolkit (NLTK)** to analyze the text in *Pride and Prejudice* and  *Frankenstein* find out the most common words. Common words like the, or and will be filtered out since it's obvious those sorts of words will be plentiful. Both of these novels came out in the 1810s and come from diametrically opposed genres: a novel of manners versus proto-science fiction, so it would be interesting to see how they used language differently at a surface level. \n",
    "\n",
    "First, the two novels will be scraped from ***Project Gutenberg***. Luckily, the site offers them in plain text form already, which skips over extracting the plain text from HTML or some other format. This means that the text can be forced into all lowercase (to avoid pride and Pride being viewed as different words), and then analyzed. Analysis will include common words, sentence length, average word length, readability, and average syllables per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['letter 1  to mrs. saville, england.',\n",
       " 'st. petersburgh, dec. 11th, 17â€”.',\n",
       " 'you will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings.',\n",
       " 'i arrived here yesterday, and my first task is to assure my dear sister of my welfare and increasing confidence in the success of my undertaking.',\n",
       " 'i am already far north of london, and as i walk in the streets of petersburgh, i feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight.',\n",
       " 'do you understand this feeling?',\n",
       " 'this breeze, which has travelled from the regions towards which i am advancing, gives me a foretaste of those icy climes.',\n",
       " 'inspirited by this wind of promise, my daydreams become more fervent and vivid.',\n",
       " 'i try in vain to be persuaded that the pole is the seat of frost and desolation; it ever presents itself to my imagination as the region of beauty and delight.',\n",
       " 'there, margaret, the sun is for ever visible, its broad disk just skirting the horizon and diffusing a perpetual splendour.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Frankenstein = requests.get(\"https://www.gutenberg.org/cache/epub/84/pg84.txt\")\n",
    "Pride = requests.get(\"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\")\n",
    "\n",
    "FrankensteinText = Frankenstein.text.replace('\\r\\n', ' ').replace('_','').lower()[1380:]\n",
    "\n",
    "PrideText = Pride.text.replace('\\r\\n', ' ').lower()[35186:]\n",
    "\n",
    "FrankensteinSent = nltk.sent_tokenize(FrankensteinText)\n",
    "PrideSent = nltk.sent_tokenize(PrideText)\n",
    "\n",
    "FrankensteinSent[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\Shmar\\AppData\\Local\\Temp\\ipykernel_13844\\3476488996.py:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['letter',\n",
       " '1',\n",
       " 'to',\n",
       " 'mrs',\n",
       " 'saville',\n",
       " 'england',\n",
       " 'st',\n",
       " 'petersburgh',\n",
       " 'dec',\n",
       " '11th']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "FrankensteinWords = tokenizer.tokenize(FrankensteinText)\n",
    "PrideWords = tokenizer.tokenize(PrideText)\n",
    "\n",
    "FrankensteinWords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes about Cleaning the Data \n",
    "\n",
    "The text needs to be all lowercase so \"doctor\" and \"Doctor\" would not be counted as separate words. This does include the possiblity of proper nounds being counted alongside their non-proper version (cooper the job versus Cooper the surname), but this is unlikely to skew the data too much. There are also many new line characters throughout the text than needs to be removed, alongside a lengthy section before the story. This was fairly easily done with the replace and lower methods for the first two issues. The introductory segement was removed by using a character counter and a few rounds of guessing and checking. The counter made this a simple task. \n",
    "\n",
    "Then a tokenizer was used to extract the words from the text, courtesy of RegEx and NLTK. This is possible without using those methods by using the split method and a space as the separator, but this is far clunkier and is more error prone.\n",
    "\n",
    "Based on the beginning of Frankenstein there are many common words like \"I\", \"and\", \"in\" etc. that should be filtered out to properly compare the two texts. Luckily, NLTK has a method for this called stopwords. It is also evident some words werestrangely written like \"_to\" which was fixed with another replace method. There are a few \"-\" as well but those may cause larger problems if they are removed without careful consideration which is likely more work than any benefit it could bring. A handful of hyphenated words or vestigial hyphens shouldn't skew the results much if at all. There is also the matter of numbers being spelled out versus as numbers ( 1 vs one), which is in the same boat. It shouldn't cause that much of a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "StrippedFrankensteinWords = [word for word in FrankensteinWords if word not in StopWords]\n",
    "StrippedPrideWords = [word for word in PrideWords if word not in StopWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['letter',\n",
       " '1',\n",
       " 'mrs',\n",
       " 'saville',\n",
       " 'england',\n",
       " 'st',\n",
       " 'petersburgh',\n",
       " 'dec',\n",
       " '11th',\n",
       " '17']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StrippedFrankensteinWords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['truth',\n",
       " 'universally',\n",
       " 'acknowledged',\n",
       " 'single',\n",
       " 'man',\n",
       " 'possession',\n",
       " 'good',\n",
       " 'fortune',\n",
       " 'must',\n",
       " 'want']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StrippedPrideWords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list comprehension worked as expected, and the text is stripped of many common words. This should be ready to be counted by the Counter container datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "FrankensteinCount = Counter(StrippedFrankensteinWords)\n",
    "PrideCount = Counter(StrippedPrideWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 208),\n",
       " ('could', 198),\n",
       " ('would', 184),\n",
       " ('yet', 152),\n",
       " ('man', 137),\n",
       " ('father', 134),\n",
       " ('upon', 128),\n",
       " ('life', 116),\n",
       " ('may', 113),\n",
       " ('every', 109)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FrankensteinCount.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mr', 782),\n",
       " ('elizabeth', 634),\n",
       " ('could', 524),\n",
       " ('would', 467),\n",
       " ('darcy', 418),\n",
       " ('said', 403),\n",
       " ('mrs', 343),\n",
       " ('much', 328),\n",
       " ('bennet', 327),\n",
       " ('must', 316)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PrideCount.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A preliminary analysis shows a general difference in tone and outlook. For example, *Pride and Prejudice* contains honorifics like Mr., Mrs. and lady amongst the most common words as well as names such as Elizabeth, Darcy, and Bennet. On the other hand, *Frankenstein* has the general terms one, father, and man as more common than specific names. This is not to see there are no clear similiarities. Both include could, would, may, much, and shall in the most used words lists. These are likely common words for the time period that happened to not be stop words in NLTK's stopwords method. There is an argument to be made that the popularity of must, much, may, might and of honorifics, titles, and names corroborates the fact that *Pride and Prejudice* is a book of manners; likewise the commonplace impersonal words like one and man corroborates the fact that *Frankenstein* is a philosophical proto-science fiction text ( this is furthered by the popularity of life,death, mind and feelings).\n",
    "\n",
    "This is interesting to analyze, but there are other aspects of the text to analyze, such as average word and sentence length, as well as readability. The unstripped version of each work will be used for this comparison. Stop words like 'and' would be useful for determining how long sentences and words are, as well as readability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.431592389222321"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = 0\n",
    "for word in FrankensteinWords:\n",
    "    characters += len(word)\n",
    "\n",
    "FrankensteinAvgWordLen = characters / len(FrankensteinWords) \n",
    "\n",
    "FrankensteinAvgWordLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.3984199269157"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = 0\n",
    "for word in PrideWords:\n",
    "    characters += len(word)\n",
    "\n",
    "PrideAvgWordLen = characters / len(PrideWords) \n",
    "\n",
    "PrideAvgWordLen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.525524585029753"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FrankensteinAvgSentLen = len(FrankensteinWords) / len(FrankensteinSent)\n",
    "FrankensteinAvgSentLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.584767100293746"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PrideAvgSentLen = len(PrideWords) / len(PrideSent)\n",
    "PrideAvgSentLen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllableCount(word):\n",
    "    vowels = [\"a\",\"e\",\"i\", \"o\",\"u\"]\n",
    "    count = 0\n",
    "    for letter in word:\n",
    "        if letter in vowels:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def TotalSyllableCount(Words):\n",
    "    count = 0\n",
    "\n",
    "    for word in Words:\n",
    "        sylcount= syllableCount(word)\n",
    "        count += sylcount\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6949304048014302"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FrankensteinTotalSyl= TotalSyllableCount(FrankensteinWords)\n",
    "FrankensteinTotalSyl / len(FrankensteinWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6684608888503034"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PrideTotalSyl= TotalSyllableCount(PrideWords)\n",
    "PrideTotalSyl / len(PrideWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an incredibly naive estimate of syllables. It misses that \"ferry\" has two syllables, and will be wrong about any word with back to back vowels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flesch_KincaidReadability(totalwords, totalsent, totalsyllables):\n",
    "    score = 206.835 - 1.015 * (totalwords / totalsent) - 84.6 * (totalsyllables / totalwords)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.550480299993836"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flesch_KincaidReadability(len(FrankensteinWords), len(FrankensteinSent), FrankensteinTotalSyl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.6996701964662"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flesch_KincaidReadability(len(PrideWords), len(PrideSent), PrideTotalSyl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally *Pride and Prejudice* scored higher on these metrics, but the results don't seem significant enough to say much. Neither novel seems signficantly more complicated than the other. A few things do need to be addressed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
